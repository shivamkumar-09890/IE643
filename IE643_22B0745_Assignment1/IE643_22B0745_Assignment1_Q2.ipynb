{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Linear activation function\n",
        "def linear(z):\n",
        "    \"\"\"\n",
        "    Computes the linear activation of input z.\n",
        "\n",
        "    Arguments:\n",
        "    z -- A scalar or numpy array.\n",
        "\n",
        "    Returns:\n",
        "    The input value z.\n",
        "    \"\"\"\n",
        "    return z\n",
        "\n",
        "# Gradient of linear activation function\n",
        "def linear_gradient(z):\n",
        "    \"\"\"\n",
        "    Computes the gradient of the linear activation function.\n",
        "\n",
        "    Arguments:\n",
        "    z -- A scalar or numpy array.\n",
        "\n",
        "    Returns:\n",
        "    A numpy array of ones with the same shape as z.\n",
        "    \"\"\"\n",
        "    return np.ones_like(z)\n",
        "\n",
        "# Example usage\n",
        "z = np.array([1, -2, 3])\n",
        "print(\"Linear activation:\", linear(z))  # Output: [1 -2 3]\n",
        "print(\"Gradient of linear activation:\", linear_gradient(z))  # Output: [1 1 1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Xnqwqzx9XoW",
        "outputId": "d58f1923-1a26-40bd-a719-8bd475cb8445"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear activation: [ 1 -2  3]\n",
            "Gradient of linear activation: [1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ReLU activation function\n",
        "def relu(z):\n",
        "    \"\"\"\n",
        "    Computes the ReLU activation of input z.\n",
        "\n",
        "    Arguments:\n",
        "    z -- A scalar or numpy array.\n",
        "\n",
        "    Returns:\n",
        "    A numpy array where each element is max(z, 0).\n",
        "    \"\"\"\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "# Gradient of ReLU activation function\n",
        "def relu_gradient(z):\n",
        "    \"\"\"\n",
        "    Computes the gradient of the ReLU activation function.\n",
        "\n",
        "    Arguments:\n",
        "    z -- A scalar or numpy array.\n",
        "\n",
        "    Returns:\n",
        "    A numpy array where gradient is 1 if z > 0, else 0.\n",
        "    \"\"\"\n",
        "    return np.where(z > 0, 1, 0)\n",
        "\n",
        "# Example usage\n",
        "z = np.array([1, -2, 3])\n",
        "print(\"ReLU activation:\", relu(z))  # Output: [1 0 3]\n",
        "print(\"Gradient of ReLU activation:\", relu_gradient(z))  # Output: [1 0 1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbYy0jjr9Xq3",
        "outputId": "a1dc09c5-681a-49ed-8b57-20d308ddf9d4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ReLU activation: [1 0 3]\n",
            "Gradient of ReLU activation: [1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define a simple deep network\n",
        "class DeepNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(DeepNetwork, self).__init__()\n",
        "        layers = []\n",
        "        for _ in range(num_layers):\n",
        "            layers.append(nn.Linear(input_size, hidden_size))\n",
        "            layers.append(nn.ReLU())\n",
        "            input_size = hidden_size\n",
        "        layers.append(nn.Linear(hidden_size, output_size))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# Example architecture\n",
        "input_size = 10\n",
        "hidden_size = 64\n",
        "num_layers = 10\n",
        "output_size = 1\n",
        "\n",
        "# Create the model\n",
        "model = DeepNetwork(input_size, hidden_size, num_layers, output_size)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Dummy data\n",
        "x = torch.randn(64, input_size)\n",
        "y = torch.randn(64, output_size)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(100):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(x)\n",
        "    loss = criterion(outputs, y)\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Check gradient norms for exploding/vanishing gradients\n",
        "    total_norm = 0\n",
        "    for p in model.parameters():\n",
        "        param_norm = p.grad.detach().data.norm(2)\n",
        "        total_norm += param_norm.item() ** 2\n",
        "    total_norm = total_norm ** 0.5\n",
        "    print(f'Epoch {epoch}, Gradient Norm: {total_norm}')\n",
        "\n",
        "    # Gradient clipping to avoid exploding gradients\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "    # Optimizer step\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSCBrUY_9Xv3",
        "outputId": "02028eda-ed5e-4224-be5a-2a11697c2e7d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Gradient Norm: 0.4523767410462905\n",
            "Epoch 1, Gradient Norm: 0.41859387022787214\n",
            "Epoch 2, Gradient Norm: 0.3879631561111073\n",
            "Epoch 3, Gradient Norm: 0.35912419756485336\n",
            "Epoch 4, Gradient Norm: 0.3324897984511198\n",
            "Epoch 5, Gradient Norm: 0.30137015990803107\n",
            "Epoch 6, Gradient Norm: 0.2712081427664562\n",
            "Epoch 7, Gradient Norm: 0.2480034852777202\n",
            "Epoch 8, Gradient Norm: 0.22443045397307265\n",
            "Epoch 9, Gradient Norm: 0.20021969507660514\n",
            "Epoch 10, Gradient Norm: 0.17710474789806716\n",
            "Epoch 11, Gradient Norm: 0.15368450088441532\n",
            "Epoch 12, Gradient Norm: 0.13202803219179884\n",
            "Epoch 13, Gradient Norm: 0.10609386827604012\n",
            "Epoch 14, Gradient Norm: 0.08347735423949314\n",
            "Epoch 15, Gradient Norm: 0.06010956516261824\n",
            "Epoch 16, Gradient Norm: 0.045946435071724655\n",
            "Epoch 17, Gradient Norm: 0.05076420832922851\n",
            "Epoch 18, Gradient Norm: 0.04771187647117514\n",
            "Epoch 19, Gradient Norm: 0.08078144057309919\n",
            "Epoch 20, Gradient Norm: 0.07998951140627654\n",
            "Epoch 21, Gradient Norm: 0.097031759145595\n",
            "Epoch 22, Gradient Norm: 0.11854996512154682\n",
            "Epoch 23, Gradient Norm: 0.13654785018719862\n",
            "Epoch 24, Gradient Norm: 0.1526206470570486\n",
            "Epoch 25, Gradient Norm: 0.17928306259132007\n",
            "Epoch 26, Gradient Norm: 0.19129421005847297\n",
            "Epoch 27, Gradient Norm: 0.21457388120106072\n",
            "Epoch 28, Gradient Norm: 0.2437402226356604\n",
            "Epoch 29, Gradient Norm: 0.28016390255869505\n",
            "Epoch 30, Gradient Norm: 0.3216129258126111\n",
            "Epoch 31, Gradient Norm: 0.36925244287166614\n",
            "Epoch 32, Gradient Norm: 0.4270972011374452\n",
            "Epoch 33, Gradient Norm: 0.49009248930841176\n",
            "Epoch 34, Gradient Norm: 0.5648724096308397\n",
            "Epoch 35, Gradient Norm: 0.6461549902809696\n",
            "Epoch 36, Gradient Norm: 0.7008706791948038\n",
            "Epoch 37, Gradient Norm: 0.6978011880797933\n",
            "Epoch 38, Gradient Norm: 0.6719167208336005\n",
            "Epoch 39, Gradient Norm: 0.6381200588417437\n",
            "Epoch 40, Gradient Norm: 0.6313912887727088\n",
            "Epoch 41, Gradient Norm: 0.6594029633068759\n",
            "Epoch 42, Gradient Norm: 0.6962341762575424\n",
            "Epoch 43, Gradient Norm: 0.5757264929211119\n",
            "Epoch 44, Gradient Norm: 0.55133318532186\n",
            "Epoch 45, Gradient Norm: 0.5844542460855482\n",
            "Epoch 46, Gradient Norm: 0.727878459227612\n",
            "Epoch 47, Gradient Norm: 0.5679645862911977\n",
            "Epoch 48, Gradient Norm: 0.5271108213616528\n",
            "Epoch 49, Gradient Norm: 0.5183482279356377\n",
            "Epoch 50, Gradient Norm: 0.614540351457761\n",
            "Epoch 51, Gradient Norm: 0.5708568614193856\n",
            "Epoch 52, Gradient Norm: 0.5151943599850919\n",
            "Epoch 53, Gradient Norm: 0.5009339986617081\n",
            "Epoch 54, Gradient Norm: 0.5322505428150344\n",
            "Epoch 55, Gradient Norm: 0.5591278129672906\n",
            "Epoch 56, Gradient Norm: 0.5038514886818197\n",
            "Epoch 57, Gradient Norm: 0.5334142414424652\n",
            "Epoch 58, Gradient Norm: 0.6230384806163926\n",
            "Epoch 59, Gradient Norm: 0.5924768024783921\n",
            "Epoch 60, Gradient Norm: 0.5625499952129207\n",
            "Epoch 61, Gradient Norm: 0.5252133178397475\n",
            "Epoch 62, Gradient Norm: 0.6249792787721827\n",
            "Epoch 63, Gradient Norm: 0.4906513248884418\n",
            "Epoch 64, Gradient Norm: 0.5244732287580135\n",
            "Epoch 65, Gradient Norm: 0.6914474034858503\n",
            "Epoch 66, Gradient Norm: 0.5180900060851344\n",
            "Epoch 67, Gradient Norm: 0.515461883289559\n",
            "Epoch 68, Gradient Norm: 0.4816438282229067\n",
            "Epoch 69, Gradient Norm: 0.4607401510456542\n",
            "Epoch 70, Gradient Norm: 0.4522416363723032\n",
            "Epoch 71, Gradient Norm: 0.5446309372561164\n",
            "Epoch 72, Gradient Norm: 0.4542830998957131\n",
            "Epoch 73, Gradient Norm: 0.7465422418744831\n",
            "Epoch 74, Gradient Norm: 0.7094391772185434\n",
            "Epoch 75, Gradient Norm: 0.37836099917002386\n",
            "Epoch 76, Gradient Norm: 0.4957321263810981\n",
            "Epoch 77, Gradient Norm: 0.9931355168777127\n",
            "Epoch 78, Gradient Norm: 0.958685321944093\n",
            "Epoch 79, Gradient Norm: 0.7235597446348736\n",
            "Epoch 80, Gradient Norm: 0.5446193273837763\n",
            "Epoch 81, Gradient Norm: 0.6664755585151446\n",
            "Epoch 82, Gradient Norm: 1.2345193525803586\n",
            "Epoch 83, Gradient Norm: 1.443545665563351\n",
            "Epoch 84, Gradient Norm: 0.4601578833821606\n",
            "Epoch 85, Gradient Norm: 1.0653751604498576\n",
            "Epoch 86, Gradient Norm: 1.3456389112117142\n",
            "Epoch 87, Gradient Norm: 0.7726684780223627\n",
            "Epoch 88, Gradient Norm: 1.7615169753438102\n",
            "Epoch 89, Gradient Norm: 1.5290009017921569\n",
            "Epoch 90, Gradient Norm: 1.3673987761046271\n",
            "Epoch 91, Gradient Norm: 0.9031576102724674\n",
            "Epoch 92, Gradient Norm: 2.045281946747569\n",
            "Epoch 93, Gradient Norm: 1.5567202381202105\n",
            "Epoch 94, Gradient Norm: 1.9243275070735049\n",
            "Epoch 95, Gradient Norm: 1.8727317059688768\n",
            "Epoch 96, Gradient Norm: 1.095534134235729\n",
            "Epoch 97, Gradient Norm: 1.0285938876913556\n",
            "Epoch 98, Gradient Norm: 1.6368966889243273\n",
            "Epoch 99, Gradient Norm: 1.4786831255128279\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define a densely connected network (like DenseNet)\n",
        "class DenseConnectedNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(DenseConnectedNetwork, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.input_size = input_size\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        # Create layers with the ability to take inputs from all previous layers\n",
        "        for i in range(num_layers):\n",
        "            self.layers.append(nn.Linear(input_size + i * hidden_size, hidden_size))\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Linear(input_size + num_layers * hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = [x]\n",
        "        for i in range(self.num_layers):\n",
        "            concat_input = torch.cat(outputs, dim=1)  # Concatenate outputs from all previous layers\n",
        "            h = torch.relu(self.layers[i](concat_input))\n",
        "            outputs.append(h)  # Store this layer's output\n",
        "        concat_final = torch.cat(outputs, dim=1)  # Final concatenation for output layer\n",
        "        return self.output_layer(concat_final)\n",
        "\n",
        "# Example architecture\n",
        "input_size = 10\n",
        "hidden_size = 64\n",
        "num_layers = 5\n",
        "output_size = 1\n",
        "\n",
        "# Create the model\n",
        "model = DenseConnectedNetwork(input_size, hidden_size, num_layers, output_size)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Dummy data\n",
        "x = torch.randn(64, input_size)\n",
        "y = torch.randn(64, output_size)\n",
        "\n",
        "# Training loop with gradient monitoring\n",
        "for epoch in range(100):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(x)\n",
        "    loss = criterion(outputs, y)\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Check gradient norms to monitor vanishing/exploding gradients\n",
        "    total_norm = 0\n",
        "    for p in model.parameters():\n",
        "        param_norm = p.grad.detach().data.norm(2)\n",
        "        total_norm += param_norm.item() ** 2\n",
        "    total_norm = total_norm ** 0.5\n",
        "    print(f'Epoch {epoch}, Gradient Norm: {total_norm}')\n",
        "\n",
        "    # Gradient clipping to avoid exploding gradients\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "    # Optimizer step\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cca8zOeZ9XzB",
        "outputId": "0065b2ff-a639-4098-dfde-141a7a9ac329"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Gradient Norm: 1.262849431997687\n",
            "Epoch 1, Gradient Norm: 0.9895228226073582\n",
            "Epoch 2, Gradient Norm: 0.8545424414365269\n",
            "Epoch 3, Gradient Norm: 0.7726841997672383\n",
            "Epoch 4, Gradient Norm: 0.690335037833154\n",
            "Epoch 5, Gradient Norm: 0.6046125273097865\n",
            "Epoch 6, Gradient Norm: 0.5332848945094729\n",
            "Epoch 7, Gradient Norm: 0.49437541775811966\n",
            "Epoch 8, Gradient Norm: 0.48796102911596906\n",
            "Epoch 9, Gradient Norm: 0.49088526416259065\n",
            "Epoch 10, Gradient Norm: 0.4920652225906443\n",
            "Epoch 11, Gradient Norm: 0.5007111581244583\n",
            "Epoch 12, Gradient Norm: 0.5161000643457264\n",
            "Epoch 13, Gradient Norm: 0.5281004086563275\n",
            "Epoch 14, Gradient Norm: 0.5271068074467375\n",
            "Epoch 15, Gradient Norm: 0.5189915161137928\n",
            "Epoch 16, Gradient Norm: 0.5154748716831468\n",
            "Epoch 17, Gradient Norm: 0.5112056764613432\n",
            "Epoch 18, Gradient Norm: 0.5002191745376299\n",
            "Epoch 19, Gradient Norm: 0.4948650537216334\n",
            "Epoch 20, Gradient Norm: 0.4909484172941085\n",
            "Epoch 21, Gradient Norm: 0.4787629475970402\n",
            "Epoch 22, Gradient Norm: 0.4636376555680408\n",
            "Epoch 23, Gradient Norm: 0.4482543755536868\n",
            "Epoch 24, Gradient Norm: 0.4302478386046645\n",
            "Epoch 25, Gradient Norm: 0.41104626864820953\n",
            "Epoch 26, Gradient Norm: 0.39602695667930427\n",
            "Epoch 27, Gradient Norm: 0.3760473411809934\n",
            "Epoch 28, Gradient Norm: 0.3575612495575791\n",
            "Epoch 29, Gradient Norm: 0.34433171304744387\n",
            "Epoch 30, Gradient Norm: 0.33897767328582246\n",
            "Epoch 31, Gradient Norm: 0.3263865515080385\n",
            "Epoch 32, Gradient Norm: 0.31829465708682125\n",
            "Epoch 33, Gradient Norm: 0.3105793811622201\n",
            "Epoch 34, Gradient Norm: 0.30417978393470424\n",
            "Epoch 35, Gradient Norm: 0.28967697335311204\n",
            "Epoch 36, Gradient Norm: 0.28013406136898256\n",
            "Epoch 37, Gradient Norm: 0.2707719304820145\n",
            "Epoch 38, Gradient Norm: 0.26060795973043244\n",
            "Epoch 39, Gradient Norm: 0.2562839297348039\n",
            "Epoch 40, Gradient Norm: 0.24413392716817153\n",
            "Epoch 41, Gradient Norm: 0.23617806127173244\n",
            "Epoch 42, Gradient Norm: 0.23109729007144647\n",
            "Epoch 43, Gradient Norm: 0.20806610909087014\n",
            "Epoch 44, Gradient Norm: 0.1903999750491821\n",
            "Epoch 45, Gradient Norm: 0.16652549903190017\n",
            "Epoch 46, Gradient Norm: 0.14305555774652887\n",
            "Epoch 47, Gradient Norm: 0.13527513217474962\n",
            "Epoch 48, Gradient Norm: 0.12863525821061442\n",
            "Epoch 49, Gradient Norm: 0.12208626616143434\n",
            "Epoch 50, Gradient Norm: 0.11975476706319027\n",
            "Epoch 51, Gradient Norm: 0.17185201964552094\n",
            "Epoch 52, Gradient Norm: 0.39039936156704363\n",
            "Epoch 53, Gradient Norm: 1.1215275252462993\n",
            "Epoch 54, Gradient Norm: 1.9669106608073008\n",
            "Epoch 55, Gradient Norm: 0.6896600991957783\n",
            "Epoch 56, Gradient Norm: 2.8862192313533033\n",
            "Epoch 57, Gradient Norm: 2.5184813589908734\n",
            "Epoch 58, Gradient Norm: 0.9235150871528999\n",
            "Epoch 59, Gradient Norm: 1.1545927621228207\n",
            "Epoch 60, Gradient Norm: 1.4263035267653\n",
            "Epoch 61, Gradient Norm: 0.9395888580096473\n",
            "Epoch 62, Gradient Norm: 1.9232611109906428\n",
            "Epoch 63, Gradient Norm: 1.9110588477566504\n",
            "Epoch 64, Gradient Norm: 0.4675417471520599\n",
            "Epoch 65, Gradient Norm: 1.445309222011069\n",
            "Epoch 66, Gradient Norm: 0.06357893246708761\n",
            "Epoch 67, Gradient Norm: 1.3220249975541676\n",
            "Epoch 68, Gradient Norm: 0.29179788564858666\n",
            "Epoch 69, Gradient Norm: 1.2430334727137873\n",
            "Epoch 70, Gradient Norm: 0.43064459369066865\n",
            "Epoch 71, Gradient Norm: 1.2233472092400952\n",
            "Epoch 72, Gradient Norm: 0.5953466211997197\n",
            "Epoch 73, Gradient Norm: 1.1779005312395028\n",
            "Epoch 74, Gradient Norm: 0.7196964466835114\n",
            "Epoch 75, Gradient Norm: 1.1153853958499418\n",
            "Epoch 76, Gradient Norm: 0.7801526786766382\n",
            "Epoch 77, Gradient Norm: 1.0172186917357215\n",
            "Epoch 78, Gradient Norm: 0.7165726796054167\n",
            "Epoch 79, Gradient Norm: 0.8804540263673064\n",
            "Epoch 80, Gradient Norm: 0.6685335029410695\n",
            "Epoch 81, Gradient Norm: 0.7480931008233026\n",
            "Epoch 82, Gradient Norm: 0.6575896146059047\n",
            "Epoch 83, Gradient Norm: 0.6032995933824719\n",
            "Epoch 84, Gradient Norm: 0.6404833128857562\n",
            "Epoch 85, Gradient Norm: 0.4774200598640614\n",
            "Epoch 86, Gradient Norm: 0.6323898534253255\n",
            "Epoch 87, Gradient Norm: 0.3511928530450692\n",
            "Epoch 88, Gradient Norm: 0.6054436991199781\n",
            "Epoch 89, Gradient Norm: 0.24573542664918524\n",
            "Epoch 90, Gradient Norm: 0.5759288291628225\n",
            "Epoch 91, Gradient Norm: 0.15170250409202413\n",
            "Epoch 92, Gradient Norm: 0.5349051628473575\n",
            "Epoch 93, Gradient Norm: 0.07851257859274349\n",
            "Epoch 94, Gradient Norm: 0.49192344848810093\n",
            "Epoch 95, Gradient Norm: 0.019303819920071824\n",
            "Epoch 96, Gradient Norm: 0.4460668236513621\n",
            "Epoch 97, Gradient Norm: 0.028126982338214195\n",
            "Epoch 98, Gradient Norm: 0.39999342604856103\n",
            "Epoch 99, Gradient Norm: 0.0585990169017924\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PMEx720g9X2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EQcEiZKo9X5i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}